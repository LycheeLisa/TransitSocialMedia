{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "import nltk\n",
    "import re\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "import pickle\n",
    "from collections import OrderedDict\n",
    "import pyLDAvis.gensim\n",
    "from gensim.models import CoherenceModel\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = pd.read_csv(r'..\\Data\\stop_words.csv')\n",
    "print(stop_words.shape)\n",
    "stop_words.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_names = pd.read_csv(r'..\\Data\\station_names.csv')\n",
    "print(station_names.shape)\n",
    "station_names.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_data_raw = pd.read_csv(r'..\\Data\\reddit_data_raw.csv')\n",
    "print(reddit_data_raw.shape)\n",
    "reddit_data_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_data_raw = pd.read_csv(r'..\\Data\\twitter_data_raw.csv')\n",
    "print(twitter_data_raw.shape)\n",
    "twitter_data_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "lemmatizer=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load updated stop words list\n",
    "stop_words = pd.read_csv(r'..\\Data\\stop_words.csv')\n",
    "stop_words = set(stop_words['stop_words']) \n",
    "\n",
    "### Load station names list\n",
    "station_names = pd.read_csv(r'..\\Data\\station_names.csv')\n",
    "station = re.compile('|'.join(map(re.escape, station_names['Station'].str.lower())))\n",
    "\n",
    "photo_names = ['svg','png','jpeg','jpg', 'photo','pictures','picture','photos']\n",
    "photo = re.compile('|'.join(map(re.escape, photo_names)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(x):\n",
    "    \"\"\"\n",
    "    Function to flatten out nested list\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    x : nested list\n",
    "    \n",
    "    Return:\n",
    "    ----------\n",
    "    [list elements removed from nested list]\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    for el in x:\n",
    "        if hasattr(el, \"__iter__\") and not isinstance(el, str):\n",
    "            result.extend(flatten(el))\n",
    "        else:\n",
    "            result.append(el)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keywords(text):\n",
    "    \"\"\"\n",
    "    Function to extract chunks of key nouns and verbs\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    text : comment string\n",
    "    \n",
    "    Return:\n",
    "    ----------\n",
    "    [list of unigram keywords ]\n",
    "    \"\"\"\n",
    "    main_phrases = []\n",
    "    for chunk in text.noun_chunks:\n",
    "        if chunk.root.dep_ == 'nsubj' or chunk.root.dep_ == 'dobj' or chunk.root.dep_ == 'pobj': \n",
    "            main_phrases.append(chunk.lemma_)\n",
    "    for word in text:\n",
    "        if word.pos_ == 'VERB':\n",
    "            main_phrases.append(word.lemma_)\n",
    "    final_phrases = flatten([i.split(' ') for i in main_phrases])\n",
    "    return [w for w in final_phrases if w not in stop_words and '-PRON-' not in w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    Function to pre-process string \n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    text : comment string\n",
    "    Return:\n",
    "    ----------\n",
    "    [processed string, [list of keywords]]\n",
    "    \"\"\"\n",
    "    ### 1. Masking common strings\n",
    "    if 'https://' in text:\n",
    "        text = re.sub(r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b', 'urllink', text, flags=re.MULTILINE)\n",
    "    processed_text = re.sub('[^A-Za-z]+', ' ', text).lower()\n",
    "    processed_text = station.sub(\"ttcstation\", processed_text)\n",
    "    processed_text = photo.sub(\"photo\", processed_text)\n",
    "    ### 2. Get Lemma and conduct POS tagging\n",
    "    input_str=nlp(processed_text)\n",
    "    lemma_str = [token.lemma_ for token in input_str]\n",
    "    filtered_str = ' '.join([w for w in lemma_str if not w in stop_words])\n",
    "    return [filtered_str, get_keywords(input_str)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_df = pd.read_csv(r'..\\Data\\reddit_data_raw.csv')\n",
    "reddit_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reddit_df['body'][0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p=mp.Pool(processes=8)\n",
    "results=p.map(tokenize,reddit_df['body'][0:50])\n",
    "p.close()\n",
    "p.join()\n",
    "whatever=list(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whatever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_list = reddit_df['body'].apply(lambda x: tokenize(x))\n",
    "\n",
    "text_data = [i[1] for i in processed_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(text_data)\n",
    "corpus = [dictionary.doc2bow(text) for text in text_data]\n",
    "# pickle.dump(corpus, open('corpus.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

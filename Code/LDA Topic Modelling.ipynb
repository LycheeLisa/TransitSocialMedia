{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "import nltk\n",
    "import re\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "import pickle\n",
    "from collections import OrderedDict\n",
    "import pyLDAvis.gensim\n",
    "from gensim.models import CoherenceModel\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import helper_functions as hf\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials, rand\n",
    "import time\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "\n",
    "### Load updated stop words list\n",
    "stop_words = pd.read_csv(r'..\\Data\\stop_words.csv')\n",
    "stop_words = set(stop_words['stop_words']) \n",
    "\n",
    "### Load station names list\n",
    "station_names = pd.read_csv(r'..\\Data\\station_names.csv')\n",
    "station = re.compile('|'.join(map(re.escape, station_names['Station'].str.lower())))\n",
    "\n",
    "photo_names = ['svg','png','jpeg','jpg', 'photo','pictures','picture','photos']\n",
    "photo = re.compile('|'.join(map(re.escape, photo_names)))         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['title', 'score', 'id', 'url', 'comms_num', 'created', 'body'], dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit_df = pd.read_csv(r'..\\Data\\reddit_data_raw.csv')\n",
    "reddit_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reddit data : 35085\n"
     ]
    }
   ],
   "source": [
    "print(\"reddit data :\", reddit_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_fraction=0.4\n",
    "reddit_df=reddit_df.sample(frac=working_fraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reddit data : 14034\n"
     ]
    }
   ],
   "source": [
    "print(\"reddit data :\", reddit_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs=8\n",
    "mp_instance=hf.mp_tokenize(df=reddit_df,\n",
    "                           target_column='body', \n",
    "                           stop_words=stop_words,\n",
    "                           station=station, \n",
    "                           photo=photo, \n",
    "                           nlp=nlp,\n",
    "                           jobs=jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "processed_list=mp_instance.excecute()\n",
    "\n",
    "end_time = time.time() - start_time\n",
    "print(\"{0} tokenized in {1} sec with {2} threads\".format(reddit_df.shape[0], end_time, jobs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = [i[1] for i in processed_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(text_data)\n",
    "corpus = [dictionary.doc2bow(text) for text in text_data]\n",
    "pickle.dump(corpus, open('corpus.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coherence_values(parameters,dictionary, corpus, texts):\n",
    "    \n",
    "    global best_score\n",
    "    global best_model\n",
    "    \n",
    "    parameters['num_topics']=int(parameters['num_topics'])\n",
    "    parameters['passes']=int(parameters['passes'])\n",
    "    \n",
    "    model = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, random_state=400, **parameters)\n",
    "    coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "    coherence_score=coherencemodel.get_coherence()\n",
    "    \n",
    "    if coherence_score > best_score:\n",
    "        best_model=model\n",
    "        best_score=coherence_score\n",
    "\n",
    "    return coherence_score\n",
    "\n",
    "def bayesian_optimizer(parameters, dictionary, corpus, texts):\n",
    "    global best_score\n",
    "    coherence_values = -compute_coherence_values(parameters, dictionary, corpus, texts)\n",
    "    if -coherence_values > best_score:\n",
    "        best_score = -coherence_values\n",
    "    return {'loss': coherence_values, 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'distributed': hp.choice('distributed', [True, False])\n",
    "# 'chunksize': hp.quniform('chunksize', 10000, 5000, 100000)\n",
    "# 'gamma_threshold':hp.loguniform('gamma_threshold', -3, 2),\n",
    "# 'minimum_phi_value':hp.loguniform('minimum_phi_value', -3, 2),\n",
    "\n",
    "trials = Trials()\n",
    "max_evals=100\n",
    "\n",
    "best_score=0\n",
    "best_model=0\n",
    "\n",
    "parameters ={'num_topics':hp.quniform('num_topics', 1, 50, 1),\n",
    "             'passes': hp.quniform('passes', 3, 30, 1),\n",
    "             'decay':hp.uniform('decay', 0.5, 1),\n",
    "             'alpha': hp.choice('alpha', [\"asymmetric\", \"auto\"])\n",
    "            }\n",
    "best = fmin(lambda x: bayesian_optimizer(parameters=x,dictionary=dictionary, corpus=corpus, texts=text_data), \n",
    "            parameters, \n",
    "            algo=tpe.suggest, \n",
    "            max_evals=max_evals, \n",
    "            trials=trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = [trials.trials[i]['result']['loss'] for i in range(len(trials.trials))]\n",
    "params = pd.DataFrame(trials.vals)\n",
    "params['loss'] = losses\n",
    "params.sort_values('loss', inplace=True)\n",
    "params.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params.to_csv(\"hyper_parameters.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "topics = best_model.print_topics(num_words=5)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lda = gensim.models.ldamodel.LdaModel.load('..\\Models\\model5.gensim')\n",
    "lda_display = pyLDAvis.gensim.prepare(lda, corpus, dictionary, sort_topics=False)\n",
    "pyLDAvis.display(lda_display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ldamodel.save('..\\Models\\model5.gensim')\n",
    "#pyLDAvis.save_html(lda_display, '..\\Visualisations\\5 topics.html')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

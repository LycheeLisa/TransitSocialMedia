{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'praw'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-cbc5a4b1ec07>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpraw\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mdt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpraw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMoreComments\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'praw'"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "from praw.models import MoreComments\n",
    "import math\n",
    "import json\n",
    "import requests\n",
    "import itertools\n",
    "import numpy as np\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "reddit = praw.Reddit(client_id='qGBDWGg0ooefOQ', \\\n",
    "                     client_secret='BXsX_M1rXMJVbRs6ED7fSJG77Bg', \\\n",
    "                     user_agent='Reddit Web Scraping', \\\n",
    "                     username='appliedintelligence', \\\n",
    "                     password='accenture2019')\t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_request(uri, max_retries = 5):\n",
    "    def fire_away(uri):\n",
    "        response = requests.get(uri)\n",
    "        assert response.status_code == 200\n",
    "        return json.loads(response.content)\n",
    "    current_tries = 1\n",
    "    while current_tries < max_retries:\n",
    "        try:\n",
    "            time.sleep(1)\n",
    "            response = fire_away(uri)\n",
    "            return response\n",
    "        except:\n",
    "            time.sleep(1)\n",
    "            current_tries += 1\n",
    "    return fire_away(uri)\n",
    "\n",
    "\n",
    "\n",
    "def pull_posts_for(subreddit, start_at, end_at):\n",
    "    \n",
    "    def map_posts(posts):\n",
    "        return list(map(lambda post: {\n",
    "            'id': post['id'],\n",
    "            'created_utc': post['created_utc'],\n",
    "            'prefix': 't4_',\n",
    "            'subreddit': subreddit\n",
    "        }, posts))\n",
    "    \n",
    "    SIZE = 500\n",
    "    URI_TEMPLATE = r'https://api.pushshift.io/reddit/search/submission?subreddit={}&after={}&before={}&size={}'\n",
    "    \n",
    "    post_collections = map_posts( \\\n",
    "        make_request( \\\n",
    "            URI_TEMPLATE.format( \\\n",
    "                subreddit, start_at, end_at, SIZE))['data'])\n",
    "    n = len(post_collections)\n",
    "    while n == SIZE:\n",
    "        last = post_collections[-1]\n",
    "        new_start_at = last['created_utc'] - (10)\n",
    "        \n",
    "        more_posts = map_posts( \\\n",
    "            make_request( \\\n",
    "                URI_TEMPLATE.format( \\\n",
    "                    subreddit, new_start_at, end_at, SIZE))['data'])\n",
    "        \n",
    "        n = len(more_posts)\n",
    "        post_collections.extend(more_posts)\n",
    "    return post_collections\n",
    "\n",
    "\n",
    "\n",
    "def give_me_intervals(start_at,end_at, number_of_days_per_interval):\n",
    "        \n",
    "    ## 1 day = 86400,\n",
    "    period = (86400 * number_of_days_per_interval)\n",
    "    end = start_at + period\n",
    "    yield (int(start_at), int(end))\n",
    "    padding = 1\n",
    "    while end <= end_at:\n",
    "        start_at = end + padding\n",
    "        end = (start_at - padding) + period\n",
    "        yield int(start_at), int(end)\n",
    "\n",
    "\n",
    "def pull_subreddit(subreddit_name, start_at, end_at):\n",
    "    subreddit = subreddit_name\n",
    "    posts = []\n",
    "    for interval in give_me_intervals(start_at, end_at,7):\n",
    "        pulled_posts = pull_posts_for(\n",
    "            subreddit, interval[0], interval[1])\n",
    "        \n",
    "        posts.extend(pulled_posts)\n",
    "        time.sleep(.500)\n",
    "    return posts\n",
    "\n",
    "subreddit_names = ['ttc', 'toronto','ontario']\n",
    "start_at = math.floor(\\\n",
    "        (datetime.utcnow() - timedelta(days=365)).timestamp())\n",
    "end_at = math.ceil(datetime.utcnow().timestamp())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "topics_dict = { \"title\":[], \n",
    "                \"score\":[], \n",
    "                \"id\":[], \n",
    "                \"url\":[], \n",
    "                \"comms_num\": [], \n",
    "                \"created\": [], \n",
    "                \"body\":[]}\n",
    "\n",
    "\n",
    "search_terms = ['ttc','presto', 'station','transit','subway', 'metrolinx','metro linx' ]\n",
    "TIMEOUT_AFTER_COMMENT_IN_SECS = .350\n",
    "\n",
    "\n",
    "final_posts = np.unique([post['id'] for post in posts ])\n",
    "\n",
    "i = 1\n",
    "for submission_id in final_posts:\n",
    "  \n",
    "    submission = reddit.submission(id=submission_id)\n",
    "  #  posts_from_reddit.append(submission)\n",
    "    submission.comments.replace_more(limit=None)\n",
    "    for comment in submission.comments.list():\n",
    "        if any(word in comment.body.lower() for word in search_terms):\n",
    "         #   comments_from_reddit.append(comment)\n",
    "            topics_dict[\"title\"].append(submission.title)\n",
    "            topics_dict[\"score\"].append(submission.score)\n",
    "            topics_dict[\"id\"].append(submission.id)\n",
    "            topics_dict[\"url\"].append(submission.url)\n",
    "            topics_dict[\"comms_num\"].append(submission.num_comments)\n",
    "            topics_dict[\"created\"].append(submission.created)\n",
    "            topics_dict[\"body\"].append(comment.body)\n",
    "        \n",
    "            if TIMEOUT_AFTER_COMMENT_IN_SECS > 0:\n",
    "                time.sleep(TIMEOUT_AFTER_COMMENT_IN_SECS)\n",
    "    print (i)\n",
    "    i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.DataFrame(topics_dict)\n",
    "final_df.to_csv('reddit_data_raw.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
